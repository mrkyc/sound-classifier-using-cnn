{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from optuna.trial import *\n",
    "import optuna\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import soundfile  # read audio\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# find the device available for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata and split file\n",
    "\n",
    "# name of the original metadata file\n",
    "metadata_file = \"esc50.csv\"\n",
    "\n",
    "# chosen categories\n",
    "categories = [\n",
    "    \"airplane\",\n",
    "    \"thunderstorm\",\n",
    "    \"vacuum_cleaner\",\n",
    "    \"cat\",\n",
    "    \"chainsaw\",\n",
    "    \"dog\",\n",
    "    \"chirping_birds\",\n",
    "    \"keyboard_typing\",\n",
    "    \"fireworks\",\n",
    "    \"church_bells\",\n",
    "]\n",
    "\n",
    "# read metadata, add backslash to filename and filter categories\n",
    "metadata = pd.read_csv(metadata_file)\n",
    "metadata[\"filename\"] = \"\\\\\" + metadata[\"filename\"]\n",
    "metadata = metadata[metadata[\"category\"].isin(categories)].reset_index(drop=True)\n",
    "\n",
    "# assign new target number to each category\n",
    "for category in categories:\n",
    "    metadata.loc[metadata[\"category\"] == category, \"target\"] = categories.index(\n",
    "        category\n",
    "    )\n",
    "\n",
    "# take the 1+ folds as training data and the first fold as validation and test data\n",
    "metadata_train = metadata[metadata[\"fold\"] > 1].reset_index(drop=True)\n",
    "metadata_val_test = metadata[metadata[\"fold\"] == 1].reset_index(drop=True)\n",
    "\n",
    "# list of metadata for validation and test data\n",
    "metadata_validation = []\n",
    "metadata_test = []\n",
    "\n",
    "# sample one item from each category for validation and test data\n",
    "# repeat until for each category there are no more items left\n",
    "for category in categories:\n",
    "    for _ in range(\n",
    "        round(metadata_val_test[metadata_val_test[\"category\"] == category].shape[0] / 2)\n",
    "    ):\n",
    "        item = metadata_val_test[metadata_val_test[\"category\"] == category].sample(2)\n",
    "        metadata_validation.append(item.iloc[0])\n",
    "        metadata_test.append(item.iloc[1])\n",
    "        metadata_val_test = metadata_val_test.drop(item.index)\n",
    "\n",
    "# convert validation and test metadata to dataframe\n",
    "metadata_validation = pd.DataFrame(metadata_validation)\n",
    "metadata_test = pd.DataFrame(metadata_test)\n",
    "\n",
    "# create category map where each target number has its corresponding category name\n",
    "category_map = (\n",
    "    metadata_test[[\"target\", \"category\"]].drop_duplicates().reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# save metadata and category map to csv\n",
    "metadata_folder = Path(\"metadata\")\n",
    "metadata_train.to_csv(metadata_folder / \"metadata_train.csv\", index=False)\n",
    "metadata_validation.to_csv(metadata_folder / \"metadata_validation.csv\", index=False)\n",
    "metadata_test.to_csv(metadata_folder / \"metadata_test.csv\", index=False)\n",
    "category_map.to_csv(metadata_folder / \"category_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class with audio utility functions for loading, augmenting and transforming audio files\n",
    "class AudioUtil:\n",
    "    # ----------------------------\n",
    "    # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        signal_tensor, sample_rate = torchaudio.load(audio_file)\n",
    "        return (signal_tensor, sample_rate)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Shifts the signal to the left or right by some percent. Values at the end\n",
    "    # are 'wrapped around' to the start of the transformed signal.\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Generate a Spectrogram\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sr = aud\n",
    "        top_db = 80\n",
    "\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels\n",
    "        )(sig)\n",
    "\n",
    "        # Convert to decibels\n",
    "        spec = torchaudio.transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return spec\n",
    "\n",
    "    # ----------------------------\n",
    "    # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "    # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "    # overfitting and to help the model generalise better. The masked sections are\n",
    "    # replaced with the mean value.\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = torchaudio.transforms.FrequencyMasking(freq_mask_param)(\n",
    "                aug_spec, mask_value\n",
    "            )\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = torchaudio.transforms.TimeMasking(time_mask_param)(\n",
    "                aug_spec, mask_value\n",
    "            )\n",
    "\n",
    "        return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for creating a dataset from the metadata files\n",
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path, augmentation):\n",
    "        self.df = df\n",
    "        self.augmentation = augmentation\n",
    "        self.data_path = str(data_path)\n",
    "        self.shift_pct = 0.4\n",
    "\n",
    "    # ----------------------------\n",
    "    # Number of items in dataset\n",
    "    # ----------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Get i'th item in dataset\n",
    "    # ----------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with\n",
    "        # the relative path\n",
    "        audio_file = self.data_path + self.df.loc[idx, \"filename\"]\n",
    "        # Get the Class ID\n",
    "        class_id = self.df.loc[idx, \"target\"]\n",
    "\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "\n",
    "        if self.augmentation:\n",
    "            aud = AudioUtil.time_shift(aud, self.shift_pct)\n",
    "\n",
    "        sgram = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "\n",
    "        if self.augmentation:\n",
    "            sgram = AudioUtil.spectro_augment(\n",
    "                sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2\n",
    "            )\n",
    "\n",
    "        return sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class with neural network model architecture\n",
    "class AudioClassifier(nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
    "        )\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
    "        )\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader_train_val(batchsize):\n",
    "    \"\"\"\n",
    "    Create data loaders for training and validation data\n",
    "    Args:\n",
    "        batchsize: batch size for training and validation data\n",
    "    Returns:\n",
    "        train_dl: training data loader\n",
    "        validation_dl: validation data loader\n",
    "    \"\"\"\n",
    "    # load metadata and create path to audio files\n",
    "    data_folder = Path(\"audio\")\n",
    "    metadata_folder = Path(\"metadata\")\n",
    "    metadata_train = pd.read_csv(metadata_folder / \"metadata_train.csv\")\n",
    "    metadata_validation = pd.read_csv(metadata_folder / \"metadata_validation.csv\")\n",
    "\n",
    "    # take only filename and target columns\n",
    "    metadata_train = metadata_train[[\"filename\", \"target\"]]\n",
    "    metadata_validation = metadata_validation[[\"filename\", \"target\"]]\n",
    "\n",
    "    # create datasets using SoundDS class\n",
    "    sound_dataset_train = SoundDS(metadata_train, data_folder, augmentation=True)\n",
    "    sound_dataset_validation = SoundDS(\n",
    "        metadata_validation, data_folder, augmentation=False\n",
    "    )\n",
    "\n",
    "    # create data loaders for training and validation data\n",
    "    # shuffle training data and do not shuffle validation data\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        sound_dataset_train, batch_size=batchsize, shuffle=True\n",
    "    )\n",
    "    validation_dl = torch.utils.data.DataLoader(\n",
    "        sound_dataset_validation, batch_size=batchsize, shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_dl, validation_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(epoch, model_state_dict, optimizer_state_dict, loss, filename):\n",
    "    \"\"\"\n",
    "    Save model checkpoint\n",
    "    Args:\n",
    "        epoch: current epoch\n",
    "        model_state_dict: state of the model\n",
    "        optimizer_state_dict: state of the optimizer\n",
    "        loss: loss of the model\n",
    "        filename: name of the file to save the checkpoint\n",
    "    \"\"\"\n",
    "    checkpoints_folder = Path(\"checkpoints\")\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model_state_dict,\n",
    "            \"optimizer_state_dict\": optimizer_state_dict,\n",
    "            \"loss\": loss,\n",
    "        },\n",
    "        checkpoints_folder / filename,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, dataloader):\n",
    "    \"\"\"\n",
    "    Calculate accuracy on test or validation data\n",
    "    Args:\n",
    "        net: model to test\n",
    "        dataloader: data loader for test or validation data\n",
    "    Returns:\n",
    "        acc: accuracy on test or validation data\n",
    "        correct_per_class: dictionary with number of correct predictions per class\n",
    "    \"\"\"\n",
    "    # initialize variables\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "    correct_per_class = {}\n",
    "\n",
    "    # set model to evaluation mode (inference mode)\n",
    "    net.eval()\n",
    "\n",
    "    # do not calculate gradients during inference\n",
    "    with torch.no_grad():\n",
    "        # iterate over test or validation data loader and calculate accuracy\n",
    "        for data in dataloader:\n",
    "            # get inputs and labels from data loader\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # normalize inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # forward pass\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # get predictions and update correct predictions and total predictions\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "            # update correct predictions per class\n",
    "            for item, pred in zip(labels, prediction):\n",
    "                item = item.item()\n",
    "                pred = pred.item()\n",
    "\n",
    "                if item in correct_per_class:\n",
    "                    correct_per_class[item] += int(pred == item)\n",
    "                else:\n",
    "                    correct_per_class[item] = int(pred == item)\n",
    "\n",
    "    # calculate accuracy\n",
    "    acc = correct_prediction / total_prediction\n",
    "\n",
    "    return acc, correct_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_optimizing(net, trial, config, epochs):\n",
    "    \"\"\"\n",
    "    Train and optimize model\n",
    "    Args:\n",
    "        net: model to train and optimize\n",
    "        trial: optuna trial\n",
    "        config: dictionary with hyperparameters\n",
    "        epochs: number of epochs to train the model\n",
    "    Returns:\n",
    "        acc_validation: accuracy on validation data\n",
    "    \"\"\"\n",
    "    # criterion for calculating loss function\n",
    "    # CrossEntropyLoss is used for classification problems with multiple classes\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # learning rate, optimizer and batch size from config dictionary with hyperparameters\n",
    "    lr = config[\"lr\"]\n",
    "    optimizer = config[\"optimizer\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    # create data loaders for training and validation data\n",
    "    train_dl, validation_dl = create_data_loader_train_val(batch_size)\n",
    "\n",
    "    # create scheduler for learning rate decay\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        steps_per_epoch=int(len(train_dl)),\n",
    "        epochs=epochs,\n",
    "        anneal_strategy=\"linear\",\n",
    "    )\n",
    "\n",
    "    # create tensorboard writer for logging and create variable for maximum validation accuracy\n",
    "    writer = SummaryWriter()\n",
    "    max_trial_accuracy = 0\n",
    "\n",
    "    # Repeat for each epoch\n",
    "    for epoch in range(epochs):\n",
    "        # create variable for running loss\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # set the model to training mode\n",
    "        net.train()\n",
    "\n",
    "        # Repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dl, 0):\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Calculate average loss over an epoch\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss_train = running_loss / num_batches\n",
    "\n",
    "        # set model to evaluation mode (inference mode)\n",
    "        net.eval()\n",
    "\n",
    "        # create variables for validation loss and number of steps\n",
    "        val_loss_validation = 0.0\n",
    "        val_steps_validation = 0\n",
    "\n",
    "        # calculate validation loss and number of steps for validation data without calculating gradients\n",
    "        for data in validation_dl:\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "                inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "                outputs = net(inputs)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss_validation += loss.cpu().numpy()\n",
    "                val_steps_validation += 1\n",
    "\n",
    "        # save checkpoint for each epoch\n",
    "        checkpoint(\n",
    "            epoch,\n",
    "            net.state_dict(),\n",
    "            optimizer.state_dict(),\n",
    "            loss,\n",
    "            \"conv_net_checkpoint.pth\",\n",
    "        )\n",
    "\n",
    "        # Calculate accuracy on validation data\n",
    "        val_accuracy = test_accuracy(net, validation_dl)[0]\n",
    "        trial.report(val_accuracy, epoch)\n",
    "\n",
    "        # update maximum trial accuracy\n",
    "        if val_accuracy > max_trial_accuracy:\n",
    "            max_trial_accuracy = val_accuracy\n",
    "\n",
    "        # check if validation accuracy is greater than maximum validation accuracy\n",
    "        # if yes, update maximum validation accuracy and save checkpoint\n",
    "        global max_val_accuracy\n",
    "        if val_accuracy > max_val_accuracy:\n",
    "            max_val_accuracy = val_accuracy\n",
    "            checkpoint(\n",
    "                epoch,\n",
    "                net.state_dict(),\n",
    "                optimizer.state_dict(),\n",
    "                loss,\n",
    "                \"conv_net_checkpoint_best.pth\",\n",
    "            )\n",
    "\n",
    "        # write to tensorboard\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss_train, epoch + 1)\n",
    "        writer.add_scalar(\n",
    "            \"Loss/validation\", val_loss_validation / val_steps_validation, epoch + 1\n",
    "        )\n",
    "        writer.add_scalar(\"Accuracy/train\", test_accuracy(net, train_dl)[0], epoch + 1)\n",
    "        writer.add_scalar(\"Accuracy/validation\", val_accuracy, epoch + 1)\n",
    "\n",
    "        # Handle pruning based on the intermediate value.\n",
    "        if trial.should_prune():\n",
    "            writer.flush()\n",
    "            writer.close()\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # flush and close tensorboard writer\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    return max_trial_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for optuna\n",
    "    Args:\n",
    "        trial: optuna trial\n",
    "    Returns:\n",
    "        acc: accuracy on validation data\n",
    "    \"\"\"\n",
    "    # create model and move it to detected device\n",
    "    net = AudioClassifier()\n",
    "    net.to(device)\n",
    "\n",
    "    # number of epochs to train the model\n",
    "    epochs = 100\n",
    "\n",
    "    # hyperparameters to optimize\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"RMSprop\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = getattr(optim, optimizer_name)(net.parameters(), lr=lr)\n",
    "\n",
    "    # create config dictionary with hyperparameters\n",
    "    config = {\"optimizer\": optimizer, \"lr\": lr, \"batch_size\": batch_size}\n",
    "\n",
    "    # train and check accuracy on validation data\n",
    "    acc = training_optimizing(net, trial, config, epochs)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:31:01,474] A new study created in memory with name: no-name-857ca2ff-2077-4410-9a5b-7184a6589db2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f165132fda490f942037a9da6996fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2023-12-11 12:40:20,381] Trial 0 finished with value: 0.9 and parameters: {'optimizer': 'RMSprop', 'lr': 0.00016878998103766953, 'batch_size': 16}. Best is trial 0 with value: 0.9.\n",
      "[I 2023-12-11 12:53:13,860] Trial 1 finished with value: 0.75 and parameters: {'optimizer': 'SGD', 'lr': 0.002769275394997339, 'batch_size': 8}. Best is trial 0 with value: 0.9.\n",
      "[I 2023-12-11 13:14:24,362] Trial 2 finished with value: 0.925 and parameters: {'optimizer': 'RMSprop', 'lr': 0.001683849132799046, 'batch_size': 32}. Best is trial 2 with value: 0.925.\n",
      "[I 2023-12-11 13:37:54,748] Trial 3 finished with value: 0.85 and parameters: {'optimizer': 'Adam', 'lr': 0.0015251613028135665, 'batch_size': 8}. Best is trial 2 with value: 0.925.\n",
      "[I 2023-12-11 14:03:23,235] Trial 4 finished with value: 0.7 and parameters: {'optimizer': 'Adam', 'lr': 0.00022989970681060707, 'batch_size': 8}. Best is trial 2 with value: 0.925.\n",
      "[I 2023-12-11 14:24:17,419] Trial 5 finished with value: 0.825 and parameters: {'optimizer': 'RMSprop', 'lr': 0.00015675360615801794, 'batch_size': 16}. Best is trial 2 with value: 0.925.\n",
      "[I 2023-12-11 14:24:28,159] Trial 6 pruned. \n",
      "[I 2023-12-11 14:24:39,207] Trial 7 pruned. \n",
      "[I 2023-12-11 14:25:26,586] Trial 8 pruned. \n",
      "[I 2023-12-11 14:25:31,542] Trial 9 pruned. \n",
      "[I 2023-12-11 14:25:35,614] Trial 10 pruned. \n",
      "[I 2023-12-11 14:26:03,651] Trial 11 pruned. \n",
      "[I 2023-12-11 14:26:08,977] Trial 12 pruned. \n",
      "[I 2023-12-11 14:43:44,180] Trial 13 finished with value: 0.925 and parameters: {'optimizer': 'RMSprop', 'lr': 0.005326548548872029, 'batch_size': 16}. Best is trial 2 with value: 0.925.\n",
      "[I 2023-12-11 14:44:06,294] Trial 14 pruned. \n",
      "[I 2023-12-11 14:44:21,527] Trial 15 pruned. \n",
      "[I 2023-12-11 14:45:03,287] Trial 16 pruned. \n",
      "[I 2023-12-11 14:45:22,679] Trial 17 pruned. \n",
      "[I 2023-12-11 14:45:37,519] Trial 18 pruned. \n",
      "[I 2023-12-11 14:45:52,486] Trial 19 pruned. \n",
      "[I 2023-12-11 14:46:10,345] Trial 20 pruned. \n",
      "[I 2023-12-11 15:06:06,346] Trial 21 finished with value: 0.85 and parameters: {'optimizer': 'RMSprop', 'lr': 0.0004901078920256551, 'batch_size': 16}. Best is trial 2 with value: 0.925.\n",
      "[I 2023-12-11 15:19:04,324] Trial 22 finished with value: 0.825 and parameters: {'optimizer': 'RMSprop', 'lr': 0.002448038706971347, 'batch_size': 16}. Best is trial 2 with value: 0.925.\n",
      "[I 2023-12-11 15:41:38,727] Trial 23 finished with value: 0.85 and parameters: {'optimizer': 'RMSprop', 'lr': 0.0007238712639562464, 'batch_size': 16}. Best is trial 2 with value: 0.925.\n",
      "[I 2023-12-11 15:41:42,707] Trial 24 pruned. \n",
      "[I 2023-12-11 15:50:47,201] Trial 25 finished with value: 0.85 and parameters: {'optimizer': 'RMSprop', 'lr': 0.004510627832034423, 'batch_size': 16}. Best is trial 2 with value: 0.925.\n",
      "[I 2023-12-11 15:50:59,717] Trial 26 pruned. \n",
      "[I 2023-12-11 15:58:41,388] Trial 27 finished with value: 0.825 and parameters: {'optimizer': 'RMSprop', 'lr': 0.0016604711845145852, 'batch_size': 32}. Best is trial 2 with value: 0.925.\n",
      "[I 2023-12-11 15:58:46,299] Trial 28 pruned. \n",
      "[I 2023-12-11 15:59:25,401] Trial 29 pruned. \n"
     ]
    }
   ],
   "source": [
    "max_val_accuracy = 0\n",
    "\n",
    "# start optuna study and optimize hyperparameters\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  30\n",
      "  Number of pruned trials:  18\n",
      "  Number of complete trials:  12\n"
     ]
    }
   ],
   "source": [
    "# get number of pruned and complete trials from study and print them\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value:  0.925\n",
      "  Params: \n",
      "    optimizer: RMSprop\n",
      "    lr: 0.001683849132799046\n",
      "    batch_size: 32\n"
     ]
    }
   ],
   "source": [
    "# print best trial and its parameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
